{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/miok/.pyenv/versions/sciseed3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n  \"This module will be removed in 0.20.\", DeprecationWarning)\n/Users/miok/.pyenv/versions/sciseed3/lib/python3.6/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora\n",
    "from gensim import models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import grid_search\n",
    "\n",
    "from classification.tools.facade import MessageManager\n",
    "from classification.tools.loader import BookManager\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset作成\n",
    "file_manager = BookManager()\n",
    "parse_manager = MessageManager(parser='cabocha')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_list = ['101', '102', '104']\n",
    "target_mapper = {'101': 1, '102': 2, '103': 3, '104': 4}\n",
    "documents = []\n",
    "labels = []\n",
    "for raw_data in raw_data_list:\n",
    "    files = file_manager.load(raw_data)\n",
    "    for file in files:\n",
    "        with open(file, 'rt') as f:\n",
    "            data = f.read()\n",
    "        message = parse_manager.extract_message(data)\n",
    "        documents.append(message.bags)\n",
    "        labels.append(target_mapper[raw_data])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dict \n",
    "# 全文書に登場する単語にidをふって辞書をつくる\n",
    "dic = corpora.Dictionary(documents)\n",
    "#dic.filter_extremes(no_below=20, no_above=0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus 作成\n",
    "# 各文書中に辞書に登録する単語が何回登場するかを数えてbag of wordsをつくる\n",
    "bow_corpus = [dic.doc2bow(d) for d in documents]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tfidf\n",
    "# 各文書の単語のtf/idfを計算する\n",
    "tfidf_model = models.TfidfModel(bow_corpus)\n",
    "tfidf_corpus = tfidf_model[bow_corpus]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 次元削減\n",
    "# 辞書から作成した6000次元ほどのコーパスを200次元まで圧縮する。\n",
    "lsi_model = models.LsiModel(tfidf_corpus, id2word=dic, num_topics=200)\n",
    "lsi_curpus = lsi_model[tfidf_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset作成\n",
    "l_novel_dict = {}\n",
    "l_novel_dict['data'] = []\n",
    "l_novel_dict['target'] = []\n",
    "l_novel_dict['target_name'] = []\n",
    "\n",
    "for doc, label in zip(lsi_curpus, labels):\n",
    "    vecs = [v[1] for v in doc]\n",
    "    l_novel_dict['data'].append(vecs)\n",
    "    l_novel_dict['target'].append(label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# トレーニングデータ作成\n",
    "X_train, X_test, y_train, y_test = train_test_split(l_novel_dict['data'], l_novel_dict['target'], random_state=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[mean: 0.36538, std: 0.00000, params: {'C': 0.001, 'gamma': 0.001, 'kernel': 'rbf'}, mean: 0.36538, std: 0.00000, params: {'C': 0.001, 'gamma': 0.01, 'kernel': 'rbf'}, mean: 0.36538, std: 0.00000, params: {'C': 0.001, 'gamma': 0.1, 'kernel': 'rbf'}, mean: 0.36538, std: 0.00000, params: {'C': 0.001, 'gamma': 1, 'kernel': 'rbf'}, mean: 0.36538, std: 0.00000, params: {'C': 0.01, 'gamma': 0.001, 'kernel': 'rbf'}, mean: 0.36538, std: 0.00000, params: {'C': 0.01, 'gamma': 0.01, 'kernel': 'rbf'}, mean: 0.36538, std: 0.00000, params: {'C': 0.01, 'gamma': 0.1, 'kernel': 'rbf'}, mean: 0.36538, std: 0.00000, params: {'C': 0.01, 'gamma': 1, 'kernel': 'rbf'}, mean: 0.36538, std: 0.00000, params: {'C': 0.1, 'gamma': 0.001, 'kernel': 'rbf'}, mean: 0.36538, std: 0.00000, params: {'C': 0.1, 'gamma': 0.01, 'kernel': 'rbf'}, mean: 0.36538, std: 0.00000, params: {'C': 0.1, 'gamma': 0.1, 'kernel': 'rbf'}, mean: 0.36538, std: 0.00000, params: {'C': 0.1, 'gamma': 1, 'kernel': 'rbf'}, mean: 0.36538, std: 0.00000, params: {'C': 1, 'gamma': 0.001, 'kernel': 'rbf'}, mean: 0.36538, std: 0.00000, params: {'C': 1, 'gamma': 0.01, 'kernel': 'rbf'}, mean: 0.36538, std: 0.00000, params: {'C': 1, 'gamma': 0.1, 'kernel': 'rbf'}, mean: 0.65659, std: 0.00777, params: {'C': 1, 'gamma': 1, 'kernel': 'rbf'}, mean: 0.36538, std: 0.00000, params: {'C': 10, 'gamma': 0.001, 'kernel': 'rbf'}, mean: 0.36538, std: 0.00000, params: {'C': 10, 'gamma': 0.01, 'kernel': 'rbf'}, mean: 0.66667, std: 0.01107, params: {'C': 10, 'gamma': 0.1, 'kernel': 'rbf'}, mean: 0.69414, std: 0.01653, params: {'C': 10, 'gamma': 1, 'kernel': 'rbf'}]\n"
     ]
    }
   ],
   "source": [
    "svc = SVC()\n",
    "cs = [0.001, 0.01, 0.1, 1, 10]\n",
    "gammas = [0.001, 0.01, 0.1, 1]\n",
    "parameters = {'kernel': ['rbf'], 'C': cs, 'gamma': gammas}\n",
    "clf = grid_search.GridSearchCV(svc, parameters)\n",
    "clf.fit(X_train, y_train)\n",
    "print(clf.grid_scores_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.712328767123\n"
     ]
    }
   ],
   "source": [
    "score = clf.score(X_test, y_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
